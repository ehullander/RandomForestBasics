{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Talking to the Trees:  How to Interpret a Decision Forest\n",
    "\n",
    "## Eric Hullander\n",
    "\n",
    "<figure>\n",
    "      <img src = 'https://poetkatehutchinson.files.wordpress.com/2012/09/treebeard1.jpg' >\n",
    "  <figcaption>“Side? I am on nobody's side, because nobody is on my side, little orc.”  - Treebeard</figcaption>\n",
    "</figure>\n",
    "\n",
    "Given a dataset and a target variable, there are many methods for finding a mapping (model) between the dataset and the target variable, and in the discipline of machine learning this process is called ‘supervised learning’.  There are two types of supervised learning, regression when our target is numerical, and classification when our target variable is categorical. Among these, there are many types of learners and each learner has its own set of strengths and weaknesses. Some attributes we would like our learner to have are: \n",
    "\n",
    "#### Qualities of a good Learner\n",
    "\n",
    "-\tInterpretability: Why did the model choose this result?  \n",
    "-\tAccuracy:   \n",
    "    -\tAre the predictions correct?   \n",
    "    -\tDo the results generalize to new data?  \n",
    "-\tEfficiency:   \n",
    "    -\tHow long does it take to train this model?  \n",
    "    -\tDoes it require lots of data  \n",
    "-\tDimensionality:   \n",
    "    -\tRobust to Irrelevancy: Do we have to perform feature selection or reduce dimensionality to obtain a good model?  \n",
    "    -\tRobust to Collinearity: Will collinear variables compromise accuracy or interpretability?  \n",
    "-\tRobust to Scaling:  Will scaling or transforming the data affect the results?  \n",
    "\n",
    "Unfortunately, we cannot always get everything we want, so when choosing a learner we must ask what kind of data we have and what questions we are trying to answer.\n",
    "\n",
    "#### The decision trees have many of these qualities.  [1]\n",
    "\n",
    "- Interpretability: We can see where splits are made \n",
    "- Accuracy: Decision trees are accurate on average (low bias), but highly dependent on the sample (high variance).  So they cannot be trusted on an individual basis\n",
    "- Efficiency:  Greedy algorithm is O(C n log n) where C are number of features and n are number of samples.\n",
    "- Robust to Dimensionality:  Dimensionality does not compromise accuracy\n",
    "- Robust to Irrelevancy: Trees perform automatic feature selection based on GINI impurity or information gain.  Irrelevant features just aren’t used\n",
    "- Robust to Collinearity: If two variables are collinear, the can use one or the other making one of the two unimportant to the model when they are equally important\n",
    "- Robust to Scaling:  Scaling and many transformations e.g. log(x) would not affect where the tree splits if using GINI impurity or information gain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://i.stack.imgur.com/r7QFy.png\" style=\"width:300px;height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about trees are they are low bias on average, the problem with decision trees is that they are highly variable individually.  Looking at the upper right target in Fig. 1, imagine if these points were obtained by building different trees on different subsets Ni of a subset Mk of the data. Then if we just averaged all of the points together, for every point above or left-of-the-target there is another point below or right-of-the-target.  We would be left with a single point near the bullseye.  If we repeated for all k subsets of M, we should obtain a cluster of points very near the bullseye.  We would now have a model that is low bias and low variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate Expected Value of binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu_X = E(X) = \\sum\\limits_{k=0}^{N}{n \\choose x}p^k (1-p)^{(N-k)} \\frac{k}{N}$$\n",
    "$$\\mu_X = p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69999999999999996"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.misc import comb\n",
    "\n",
    "p = .7\n",
    "tot = 0\n",
    "N = 10\n",
    "for k in range(N+1):\n",
    "    tot += comb(N,k)*(p**k)*((1-p)**(N-k))*1.0*k/N\n",
    "tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu_X = E(X) = \\sum\\limits_{k=0}^{N}{n \\choose x}p^k (1-p)^{(N-k)} (\\frac{k}{N} - p)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "[wins and losses example], \n",
    "number of wins, \n",
    "number of combinations, \n",
    "probability of number of wins, \n",
    "distance from mean'''\n",
    "result = []\n",
    "\n",
    "for N in range(1,50):\n",
    "    tot = 0\n",
    "    for k in range(N + 1):\n",
    "        x = np.hstack((np.zeros(N-k), np.ones(k)))\n",
    "        #print x, k, comb(N,k), ((p**k)*((1-p)**(N-k))), (1.0*k/N - p)\n",
    "        tot += (comb(N,k)*((p**k)*((1-p)**(N-k))))*(1.0*k/N - p)**2\n",
    "    result.append(tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Alternatively\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "Theorem. Suppose $$X_1, X_2, ... , X_n$$ are n independent random variables with means $$\\mu_1,\\mu_2,⋯,\\mu_n $$and variances $$\\sigma_1,\\sigma_2,⋯,\\sigma_n $$. \n",
    "\n",
    "Then, the mean and variance of the linear combination $$Y = \\sum\\limits_{i=1}^{n} a_i X_i$$, where a1, a2, ... , an are real constants are:\n",
    "\n",
    "$$\\mu_Y = \\sum\\limits_{i=1}^{n} a_i \\mu_i$$and:\n",
    "\n",
    "$$\\sigma^2_Y = \\sum\\limits_{i=1}^{n} a^2_i \\sigma^2_i$$\n",
    "\n",
    "respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# above equation, ai = 1/n, sigma is p*(1-p).  All factor out of summation.  Summation reduces to n\n",
    "#becomes p*(1-p)/n\n",
    "result2 = 1.0*p*(1-p)/np.array(range(1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1102ce9d0>]"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noS8iEiL/H0e6eVDIDGc6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1102cec90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(result)\n",
    "plt.plot(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21000000000000002"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p*(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
